---
layout: post
title: Convolutional Neural Networks
categories: NeuralNets
---

Previous: Introduction II to Neural Networks [here](/blog/neuralnets/2017/06/05/intro-p2-neural-networks/)

## 1. Convolutional Neural Networks

There have been numerous applications of convolutional neural networks going back to the early 1990s, but it was since the early 2000s when CNNs have been applied with great success to detection, segmentation and recognition of objects and regions in images. Recently, they have achieved major results in face recognition [1], speech recognition [2] and raw audio generation [3]. The model presented in [3] by *DeepMind*, which inspired us to undertake this work, also reaches state-of-the-art performance in text-to-speech applications.

Despite these successes, CNNs were largely forsaken by the mainstream computer-vision and machine-learning communities until the *ImageNet* competition in 2012. The spectacular results achieved by A.Krizhevsky, I.Sutskever and G.Hinton [4] came from the efficient use of GPUs, ReLUs, a new regularization technique to avoid overfitting called dropout, and techniques to generate even more training examples by deforming the existing ones. This success has brought about a revolution in computer vision; CNNs are now the dominant solution for almost all recognition and detection tasks and approach human performance on some others [5].

<br />
#### The Convolution Operation

The operation used in a convolutional neural network does not correspond precisely to the definition of convolution as used in other fields such as engineering or pure mathematics. The convolution of two real-valued functions is typically denoted with an asterisk $$(∗)$$ and it is defined as the integral of the product of the two functions after one is reversed and shifted. However, working with data on a computer, time is usually discretized and it can take only integer values. Thus, if we assume that f and k are two discrete functions defined only on integer $$n$$, we can then define the discrete convolution as:

$$
\begin{equation}
	\label{eq:conv}\tag{1.1}
	s(n) \equiv \sum_{m=-\infty}^{\infty}{f(m)k(n-m)} =  \sum_{m=-\infty}^{\infty}{f(n-m)k(m)}.
\end{equation}
$$

In convolutional network terminology, the first argument to the convolution is often referred to as the *input* (function $$f$$) and the second argument as the *filter* or *kernel* (function $$k$$). Both of them are multidimensional arrays, or tensors, that are zero everywhere but the finite set of points for which we store the values. This means that in practice we can implement the infinite summation as a summation over a finite number of array elements.

The output *s* can be referred to as the *feature map*, which usually corresponds to a very sparse matrix (a matrix whose entries are mostly equal to zero) [6, ch.9, pp.333-334]. This is because the kernel is usually much smaller than the input image.

The only reason to flip the second argument in Equation \eqref{eq:conv} is to obtain the commutative property. Since in neural networks the kernel is symmetric, commutative property is not usually important and many neural network libraries implement a pseudo-convolution without reversing the kernel, known as *cross-correlation*.

$$
\begin{equation}
	\label{eq:corr} \tag{1.2}
	s(n) \equiv \sum_{m}{f(m)k(m+n)}.
\end{equation}
$$

It can be easily generalized for a two-dimensional input $$F:\mathbb{Z}^{2} \rightarrow \mathbb{R}$$, which probably will be used with a two-dimensional kernel $$K:\Omega_{r} \rightarrow \mathbb{R}$$, with $$\Omega_{r} = [-r,r]^{2} \cap \mathbb{Z}^{2}$$ [7]:

$$
\begin{equation}
\label{eq:corr2d} \tag{1.3}
S(p) = (F*K)(p) \equiv \sum_{m+n=p}{F(m)K(n)}.
\end{equation}
$$

---

A **CNN** or **ConvNet** (*Figure 1.1.*) can be regarded as a variant of the standard neural network. It is a feedforward network, i.e., each layer receives inputs only from the previous layer, so information is always traveling forward. Its typical architecture is structured as a series of stages. The first few stages consists of alternating so-called convolution and pooling layers, instead of directly using fully connected hidden layers like in RNNs.

![image]({{ site.baseurl }}/public/photos/cnn.jpg){: .center-image }

*Figure 1.1. A simple convolutional neural network. (Source: [www.clarifai.com](www.clarifai.com))*

CNNs make the explicit assumption that the input data is organized as a number of feature maps. This is a term borrowed from image processing applications, in which it is intuitive to organize the input as a two-dimensional array (for color images, RGB values can be viewed as three different 2D feature maps). Thus, the layers of a CNN have neurons arranged in three dimensions: width, height and depth. For example, input images in CIFAR-10 are an input volume of activation which has dimensions $$32\times32\times3$$ (width, height and depth respectively) as shown in *Figure 1.2.* 

![image]({{ site.baseurl }}/public/photos/cnn_dim.jpg){: .center-image }

*Figure 1.2. One of the hidden layers show how three dimensions are arranged in a CNN. Every layer transforms the 3D input volume to a 3D output volume of neuron activations through a differentiable function. (Source: [cs231n.github.io](cs231n.github.io/convolutional-networks/))*

There are four key concepts behind CNNs that take advantage of the properties of natural signals: local connections, shared weights and biases, pooling and the use of many layers [5]. The idea of stacking many layers up will be explained in the next post, introducing the advantages of using deep neural networks.

**1) Local connections.** In CNNs not every input sample is connected to every hidden neuron, as well as it is impractical to connect neurons to all neurons in the previous layer. Instead, connections are made in small, localized regions of the input feature map known as receptive field. To be more precise, each neuron in the first hidden layer is connected to a small region of the input neurons, say, for example, a 3×3 region as in *Figure 1.3.* We then slide the local receptive field across the entire input, so for each local receptive field, there is a different hidden neuron in the first hidden layer. We can think of that particular hidden neuron as learning to analyze its particular local receptive field.

![image]({{ site.baseurl }}/public/photos/local.jpg){: .center-image }

*Figure 1.3. Connections for a particular neuron in the first hidden layer. Its receptive field is highlighted in pink.*

**2) Shared weights and biases.** Shared weights and bias are often said to define a kernel or filter (different weights led to different filters). Following the example above, each hidden neuron has a bias and 3x3 weights connected to its local receptive field. But this bias and weights are the same for every neuron on each layer. This means that all the neurons in the first hidden layer detect exactly the same feature, just at different locations in the 2D input array. A big advantage of sharing weights and biases is that it greatly reduces the number of parameters involved in a convolutional network. Despite the runtime of forward propagation remains the same, the storage requirements are vastly reduced. 

**3) Pooling.** A pooling layer is a form of non-linear downsampling and it is usually used immediately after convolutional layers. Pooling layers condense the information in the output from the convolutional layer by replacing the output of the net at a certain location with a summary statistic of the nearby outputs [6]. As a concrete example, one common procedure for pooling is known as *max-pooling* where the maximum output within a rectangular neighborhood is reported. Another popular pooling functions is *L2*, which takes the square root of the sum of the squares of the activations in the region applied. 

<br />
#### Dilated Convolution

In dense prediction problems such as semantic segmentation or audio generation, working with a large receptive field is an important factor in order to obtain state-of-the art results. In [7], a new convolutional network module that is specifically designed for dense prediction is defined. It is known as *dilated* or *atrous* convolution, a modified version of the standard convolution. Let $$l$$ be a dilation factor and let $$*_{l}$$ be defined as in Equation \eqref{eq:dil} for a two-dimensional input:

$$
\begin{equation}
	\label{eq:dil} \tag{1.4} 
	(F *_{l} K)(p) \equiv \sum_{m+\textit{l}n=p}{F(m)K(n)}.
\end{equation}
$$

A dilated convolution is a convolution where the kernel is applied over an area larger than its length by skipping input values with a certain step [3], also called dilation factor. It effectively allows an exponential expansion of the receptive field without loss of resolution or coverage. This is similar to pooling or strided convolutions, but here the output has the same size as the input. Note as a special case, dilated convolution with dilation 1 yields the standard convolution.

<br />
### References

[1] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing the gap to human-level performance in face verification”, *IEEE Conference on Computer Vision and Pattern Recognition*, 2014, pp. 1701–1708.

[2] O. Abdel-Hamid, A.-R. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, “Convolutional neural networks for speech recognition”, *IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)*, vol. 22, no. 10, pp. 1533–1545, 2014.

[3] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “Wavenet: A generative model for raw audio,” *CoRR*, vol. abs/1609.03499, 2016.

[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” *Advances in Neural Information Processing Systems (NIPS)*, pp. 1097–1105, 2012. 

[5] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521, no. 7553, pp. 436–444, 2015.

[6] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. *MIT Press*, 2016, [www.deeplearningbook.org](www.deeplearningbook.org).

[7] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated convolutions,” *CoRR*, vol. abs/1511.07122, 2015.






